Several things that should probably be tested...
---

Sensitivity analysis/optimization on all metaparameters
- including bounds

Test cross-initialization (naive bounds w/ parsimonious perturbations, vice-versa)
- want to establish that this isn't a case of better initialization

Test alternative bounding logic
- truncating
- bouncing
- wrapping

Show that parsimonious can rescue naive but not vice-versa

SVD on resulting parameters?

Assert that the problem is not solvable by convex optimizers

Assert that the problem is not solvable by diff evo

Test
- All combinbations of
	- naive vs. parsimonious bounds
	- naive vs. parsimonious perturbations
	- ordinal vs. random-direction perturbations
- naive-naive-ordinal and parsi-parsi-ordinal in paper
	- not certain but pretty confident that the ranking is
	- naive-naive-ordinal
	- naive-naive-random
	- naive-parsi-*/parsi-naive-*
	- parsi-parsi-ordinal
	- unsure about parsi-parsi-random

Simulated annealing
- I *really* don't want to do this; adds new metaparameters
- will probably improve naive perturbations but that's beside the point

Other tasks
---

Primary and secondary estimation interface
- primary: parameter-heavy, logic-light
- secondary: parameter-light, logic-heavy, calls primary

Move naive bounds constants to a file

Various repo cleanup tasks
- moved/removed a lot of files
- still needs restructuring

File documentation

Function documentation

Readme

Tests

Review modules under 'utils' and consider removing

Done
----

Remove redundant objective calculations
- Shouldn't be slowing things down much (only when logging) but still not good

Clean up constants
- Lots of constants that are set to unity

Possible major issue: naive opt has really poor init.
- Appears to be a regularization problem, as the data is well-fit.
- Just checked - this was NOT true in the past.  I don't know of a solution outside using the parsimonious initial values for the naive opt, or similar.
- Only change I can tell was made was to increase the number of iterations on the linprog solver.  However, reverting this did not fix the problem.
- pyenv does not seem to matter.
- data_agnostic (vs all_scaled) also has this issue
- the parameters themselves seem fine but some of the activities are extreme
- biggest problem seems to be the enolase reverse reaction
- I can't seem to associate this with any particular diff.  I *did* make some changes to metaparameters but *not* to bounds or initialization.
- Theories:
	- Past results were never correct
		False, checked history and validated that the diseq error was low
	- Changes to solver arguments
		Probably false, can't find anything in recent git history
	- NumPy/SciPy version issues
		Probably false, there aren't any differences b/t pyenv parest and parest2
	- Differences in batch vs. solo execution
		Nothing suspicious here
	- Changes missing from git
		Can't prove.  There *are* differences between my backup and working version;
		both are still wrong but wrong in different ways.
	- Optimization metaparameter changes
		False, metaparameters don't come into play until after init
	- Bounds changes
		This is the most likely cause IMO.
	- Previously used parsi bounds for init
		False, gives same result as parsimonious.
	- Random direction perturbation changes
		Tested, no difference
	- Training data changed
		Probably false, init fit appears to be the same
	- Sign changes on some bounded values
		Metabolite, enzyme concentrations appear fine
		Kcat and KM also appear to be fine
		Probably false, would cause bigger init. issues
- Notes
	- current naive init f = 7.57, g = 8.01e19
	- current parsi init f = 7.57, g = 111.4
	- old naive init (*iteration 6) f = 7.57, g = 2.55
	- old parsi init (*iteration 1206) f = 7.61, g = 96.0
		Unfortunately it looks like I didn't save the initial value in the history, only subsequent changes.  Irritating.
	- backup naive init f = 7.57, g = 1.14e12
	- backup parsi init f = 7.57, g = 124.0
		Backup is similar yet clearly distinct.
	- pgi and gap are specifically misbehaving (very negative v's)
		these are two of the three (including pps) reactions that lack equi data
	- removing kcatf bounds makes g worse (2.76e23)
	- adding kcatr bounds makes g reasonable (3.91)
	- backup vs current: init_matrix row order has changed
	- there are other difference b/t backup and current, all appear to be subsequent of ordering differences
- Unless I have some profound insight, I've decided to force the naive optimizations to use the parsimonious optimizations' initial values.  This seems to be the most equitable choice.
- I *should* reevaluate the naive optimizations but I really don't want to
